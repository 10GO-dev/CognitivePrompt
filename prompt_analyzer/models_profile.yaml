model_profiles:
  grammar_n_coherence:
    model_name: DeepSeek-V3
    version: 2024-05-01
    parameters:
      max_tokens: 300       # Maximum number of tokens to generate
      temperature: 0       # Sampling temperature for randomness
      top_p: 1             # Nucleus sampling probability
      frequency_penalty: 0.0 # Penalize repeated tokens
      presence_penalty: 0.0  # Encourage new topics

  filter_context_evaluation:
    model_name: GPT-4-turbo
    version: "2024-05-01"
    parameters:
      max_tokens: 512         # Enough tokens to provide a detailed evaluation summary
      temperature: 0.25       # Low randomness for consistent, precise output
      top_p: 0.9              # Ensures most likely tokens are chosen
      frequency_penalty: 0.1  # Slight penalty to reduce repetitiveness
      presence_penalty: 0.1   # Encourages the generation of relevant new information

  prompt_optimization:
    model_name: GPT-4-turbo
    version: "2024-05-01"
    parameters:
      max_tokens: 512         # Sufficient tokens to output the fully optimized prompt
      temperature: 0.2        # Lower temperature for clear, concise responses
      top_p: 0.9              # Nucleus sampling for focused output
      frequency_penalty: 0.1  # To prevent repetitive phrases
      presence_penalty: 0.1   # To encourage necessary fresh content

    